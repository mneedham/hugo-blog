+++
draft = false
date="2024-04-07 00:44:37"
title="Semantic Router: Stop LLM chatbots going rogue"
tag=['semantic-router', 'llama.cpp', 'generative-ai', 'til']
category=['TIL']
description="In this post, we'll learn how to use Semantic Router to stop LLM-based chatbots going off the rails"
image="uploads/2024/03/llamacpp-banner.png"
+++

:icons: font

A tricky problem when deploying LLM-based chatbots is working out how to stop them talking about topics that you don't want them to talk about.
Even if 

[source, bash]
----
CMAKE_ARGS="-DLLAMA_METAL_EMBED_LIBRARY=ON -DLLAMA_METAL=on" pip install -U llama-cpp-python --no-cache-dir
----


[source, python]
----
from llama_cpp import Llama
import sys
import readline


def call_llm(model, content):
  return model.create_chat_completion(
    messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": content}
    ],
    stream=True
  )


model = Llama(
  model_path="./mistral-7b-instruct-v0.1.Q4_K_M.gguf",
  n_gpu_layers=-1,
  n_ctx=2048,
  verbose=False
)

print("How can I help you?")
while True:
  user_input = input("\n>>> ")
  if user_input in ["/bye", "exit"]:
    sys.exit(1)
  response = call_llm(model, user_input)
  for chunk in response:
    print(chunk['choices'][0]['delta'].get(
        'content', ''), end='', flush=True)

----